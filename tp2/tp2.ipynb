{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images Shape: (60000, 28, 28), Train Labels Shape: (60000,)\n",
      "Test Images Shape: (10000, 28, 28), Test Labels Shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "# Chemins des fichiers\n",
    "DATA_DIR = \"/Users/mariusavosse/Documents/M2_Paris_Cité/S2/DeepLearing/tp2/fashion\"\n",
    "TRAIN_IMAGES_FILE = os.path.join(DATA_DIR, \"train-images-idx3-ubyte.gz\")\n",
    "TRAIN_LABELS_FILE = os.path.join(DATA_DIR, \"train-labels-idx1-ubyte.gz\")\n",
    "TEST_IMAGES_FILE = os.path.join(DATA_DIR, \"t10k-images-idx3-ubyte.gz\")\n",
    "TEST_LABELS_FILE = os.path.join(DATA_DIR, \"t10k-labels-idx1-ubyte.gz\")\n",
    "\n",
    "TEXT_LABELS = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat', \n",
    "               'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "\n",
    "# Fonctions utilitaires pour charger les données\n",
    "def load_images(file_path):\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        magic, num_images, rows, cols = np.frombuffer(f.read(16), dtype=np.uint32, count=4)\n",
    "        num_images, rows, cols = num_images.byteswap(), rows.byteswap(), cols.byteswap()\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return images.reshape(num_images, rows, cols) / 255.0\n",
    "\n",
    "def load_labels(file_path):\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        magic, num_labels = np.frombuffer(f.read(8), dtype=np.uint32, count=2)\n",
    "        num_labels = num_labels.byteswap()\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return labels\n",
    "\n",
    "# Chargement des datasets\n",
    "train_imgs = load_images(TRAIN_IMAGES_FILE)\n",
    "train_labels = load_labels(TRAIN_LABELS_FILE)\n",
    "test_imgs = load_images(TEST_IMAGES_FILE)\n",
    "test_labels = load_labels(TEST_LABELS_FILE)\n",
    "\n",
    "# Vérification\n",
    "print(f\"Train Images Shape: {train_imgs.shape}, Train Labels Shape: {train_labels.shape}\")\n",
    "print(f\"Test Images Shape: {test_imgs.shape}, Test Labels Shape: {test_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images Flat Shape: (60000, 784)\n",
      "Test Images Flat Shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "## Prétraitement des données\n",
    "### Aplatir les images\n",
    "train_imgs_flat = train_imgs.reshape(train_imgs.shape[0], -1)  # (60000, 784)\n",
    "test_imgs_flat = test_imgs.reshape(test_imgs.shape[0], -1)    # (10000, 784)\n",
    "\n",
    "print(f\"Train Images Flat Shape: {train_imgs_flat.shape}\")\n",
    "print(f\"Test Images Flat Shape: {test_imgs_flat.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implémentation de Softmax\n",
    "def softmax(logits):\n",
    "    max_logits = np.max(logits, axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits - max_logits)\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perte d'Entropie Croisée\n",
    "def cross_entropy_loss(probabilities, labels):\n",
    "    return -np.mean(np.log(probabilities[np.arange(len(labels)), labels]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calcul des Gradients\n",
    "def compute_gradients(x, probabilities, labels):\n",
    "    probabilities[np.arange(len(labels)), labels] -= 1\n",
    "    grad_W = x.T @ probabilities / len(labels)\n",
    "    grad_b = np.sum(probabilities, axis=0) / len(labels)\n",
    "    return grad_W, grad_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.2885\n",
      "Epoch 50, Loss: 0.4300\n",
      "Epoch 100, Loss: 0.2463\n",
      "Epoch 150, Loss: 0.1690\n",
      "Epoch 200, Loss: 0.1267\n",
      "Epoch 250, Loss: 0.1004\n",
      "Epoch 300, Loss: 0.0827\n",
      "Epoch 350, Loss: 0.0701\n",
      "Epoch 400, Loss: 0.0607\n",
      "Epoch 450, Loss: 0.0534\n",
      "Final Loss: 0.0478\n"
     ]
    }
   ],
   "source": [
    "## Entraînement sur un Batch Unique\n",
    "\n",
    "# Initialisation\n",
    "np.random.seed(42)\n",
    "W = np.random.normal(0, 0.01, (784, 10))\n",
    "b = np.zeros(10)\n",
    "\n",
    "learning_rate = 0.1\n",
    "batch_size = 64\n",
    "x_batch = train_imgs_flat[:batch_size]\n",
    "y_batch = train_labels[:batch_size]\n",
    "\n",
    "# Entraînement\n",
    "for epoch in range(500):\n",
    "    logits = x_batch @ W + b\n",
    "    probabilities = softmax(logits)\n",
    "    loss = cross_entropy_loss(probabilities, y_batch)\n",
    "    grad_W, grad_b = compute_gradients(x_batch, probabilities, y_batch)\n",
    "    W -= learning_rate * grad_W\n",
    "    b -= learning_rate * grad_b\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Vérification de la perte\n",
    "print(f\"Final Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4903\n",
      "Epoch 2, Loss: 0.4924\n",
      "Epoch 3, Loss: 0.4403\n",
      "Epoch 4, Loss: 0.4441\n",
      "Epoch 5, Loss: 0.4192\n",
      "Epoch 6, Loss: 0.4302\n",
      "Epoch 7, Loss: 0.4269\n",
      "Epoch 8, Loss: 0.4133\n",
      "Epoch 9, Loss: 0.4193\n",
      "Epoch 10, Loss: 0.4410\n"
     ]
    }
   ],
   "source": [
    "## Entraînement Complet avec Mini-Batchs\n",
    "def train_model(train_data, train_labels, epochs=10, batch_size=64, lr=0.1):\n",
    "    global W, b\n",
    "    for epoch in range(epochs):\n",
    "        # Mélanger les données\n",
    "        indices = np.arange(len(train_data))\n",
    "        np.random.shuffle(indices)\n",
    "        train_data = train_data[indices]\n",
    "        train_labels = train_labels[indices]\n",
    "\n",
    "        # Parcourir les mini-batchs\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            x_batch = train_data[i:i+batch_size]\n",
    "            y_batch = train_labels[i:i+batch_size]\n",
    "            logits = x_batch @ W + b\n",
    "            probabilities = softmax(logits)\n",
    "            grad_W, grad_b = compute_gradients(x_batch, probabilities, y_batch)\n",
    "            W -= lr * grad_W\n",
    "            b -= lr * grad_b\n",
    "\n",
    "        # Calculer et afficher la perte après chaque epoch\n",
    "        logits = train_data @ W + b\n",
    "        probabilities = softmax(logits)\n",
    "        loss = cross_entropy_loss(probabilities, train_labels)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Entraînement\n",
    "train_model(train_imgs_flat, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 84.90%\n",
      "Test Accuracy: 83.27%\n"
     ]
    }
   ],
   "source": [
    "## Evaluation\n",
    "def evaluate_model(data, labels):\n",
    "    logits = data @ W + b\n",
    "    probabilities = softmax(logits)\n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    return accuracy\n",
    "\n",
    "# Évaluation\n",
    "train_accuracy = evaluate_model(train_imgs_flat, train_labels)\n",
    "test_accuracy = evaluate_model(test_imgs_flat, test_labels)\n",
    "print(f\"Train Accuracy: {train_accuracy:.2%}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
